\documentclass[12pt,a4paper,twoside]{report}

\include{settings}

\newcommand{\studyprogramme}{Media Management}
\newcommand{\degreetype}{Bachelor of Arts}
\newcommand{\thesistitle}{
The System and Software Architecture of Back-end Applications for the Short-term Rental of Electric Scooters
}
\newcommand{\thesisauthor}{Jakob Löhnertz}
\newcommand{\thesisdate}{23. Juli 2018}
\newcommand{\thesislocation}{Wiesbaden}
\newcommand{\firstmarker}{Prof.\ Dr.\ Johannes Luderschmidt}
\newcommand{\secondmarker}{Merle Hiort}

\begin{document}

\include{preamble}


\begin{abstract}

The abstract will be added at this place towards the end of the thesis.

\end{abstract}


\tableofcontents



\chapter{Introduction} \label{chap:intro}

\epigraphhead[55]{\epigraph{Beauty is more important in computing than anywhere else
in technology because software is so complicated. Beauty is the ultimate defense against complexity.}
{\textit{David Gelernter}}}


The definition of the word \emph{scooter} is fairly vague.
Some describe it as a small vehicle with a handlebar and a platform to stand on,
the kind traditionally children were riding.
However, this paper looks at the \emph{motorcycle-esque type} of a scooter, and in
particular the electric version of it.

Many pain points of internal combustion engines vanish with the switch to their electric
counterparts. The maintenance of the mechanical system of the whole vehicle becomes easier.
There are less moving parts which makes for less errors.
On the downside, the exchange of the fuel causes the most problems with electric engines.
Of course they utilize batteries instead of fossil fuels as their impetus.
Although the invention of the battery is older than that of the
internal combustion engine\cn, the technology is still not perfected and sees new
advancements every year. Alas, it also combats many challenges in terms of
safety, cost, capacity, weight, recycling, speed of recharging and longevity.

For the provider, the handling of a short-term rental service
only becomes feasible with the automation of as many processes as possible.
This requires a back-end application ideally paired with a front-end.
Most of the time, ongoing human resource costs make up the biggest percentage
in the expenses of any business.\cn Thus, this demands for less human workforce
as part of the processes of a rental service.
Meanwhile, the upfront expenditures for the software development of
an application are still vast but in the long run, the running expenses are less.
The only cost driver afterwards is the maintenance of the created software.
As a consequence, the question should be how to mitigate the maintenance costs
of a software product. The main aspect is a proper application of
software engineering which is defined as:
\begin{displayquote}
\emph{"The application of a systematic, disciplined, quantifiable approach to the
development, operation, and maintenance of software."}~\cite{se-ieee}
\end{displayquote}
As a consequence, section \ref{sect:software-development-process} exhibits
said approach in great detail.

Furthermore, the fact that the scooters are always connected to the internet
as well as the nature of current batteries desire a reactive and faultless application.

Finally, the corporate partner of this thesis, which is discussed in detail
in section \ref{sect:e-bility}, is already in possession of a solution.
Therefore, the subject of extending an existing back-end application
with missing features is also targeted.

In conclusion, this leads into the research question of this bachelor thesis:\\
\emph{What does the system and software architecture of a back-end application
for the short-term rental of electric scooters look like and which considerations
need to be made to integrate additional functionality into an
existing legacy software solution for that purpose?}



\chapter{Fundamentals} \label{chap:fundamentals}

\epigraphhead[55]{\epigraph{The danger in the sequence [waterfall approach]
is that the project moves from being grand to being grandiose,
and exceeds our human intellectual capabilities for management and control.}
{\textit{Harlan D. Mills}}}


\section{The market for the rental of electric scooters} \label{sect:electric-scooter-market}

The market for the short-term rental of motor vehicles in Germany
is currently growing rapidly~\cite{bundesverband-carsharing-statistics}.
The situation on the market is no different for medium and long-term rentals~\cite{sparkasse-kfz-vermietung}.
However, this sector is dominated by cutthroat competition.
It has been dictated by large corporations such as \textit{Sixt}, \textit{Hertz},
\textit{Avis} and \textit{Europcar} for years~\cite{sparkasse-kfz-vermietung}.
Although smaller providers exist, this market has become an oligopoly and
the barriers for a market entry are high for new types of providers.

In contrast, the market for short-term rental, also known as \textit{car sharing} for cars,
is a comparatively young market. It has only existed in Europe in its breadth
since the late 1980s~\cite{history-of-carsharing} and in its current form
only since the advent of the smartphone, since many offers require the use of a mobile app.
For this reason the market for short-term rentals and thus its saturation is significantly lower.
According to the \textit{Bundesverband CarSharing e.V.}, the current total number of cars
in the car sharing sector in Germany is about 17,200. In contrast, almost 10\% of all
new vehicle registrations in Germany in 2015 were attributable to traditional
car rental companies such as the above~\cite{sparkasse-kfz-vermietung}.
This represents an increase of 339,000 vehicles, in similar fashion every year.
By contrast, the share of vehicles with electric drive is again lower at
just under 10\% of the total number of car sharing vehicles.
Electric scooters do not even appear in the statistics of the
\textit{Bundesverband CarSharing e.V.}~\cite{bundesverband-carsharing-statistics}.

However, the market in other nations has already experienced strong growth;
one example is the Chinese market. Around 20 million electric scooters were sold
there in 2016 alone~\cite{heise-electric-scooters}. Market value forecasts are also promising.
Last year, the US company \textit{P\&S Market Research} predicted an annual growth of
just under 7\% in the market for motorized two-wheeled vehicles with
electric drive until 2025~\cite{pands-electric-scooters}.
The future of the market is promising, as more and more nations around the world
are deciding on a ban on vehicles with internal combustion engines for the
foreseeable future~\cite{faz-combustion-engine-ban}.

Nevertheless, there are already working business models in European cities
which can yield additional information.
According to the \textit{Global Scootersharing Market Report 2017} by the
\textit{Innovation Centre for Mobility and Societal Change (InnoZ) GmbH},
the short-term rental of scooters in general consist 92\% of scooters with
electric drive. Additionally, 95\% are so-called \textit{free-floating systems},
meaning that each customer can drop off the scooters wherever they want, regardless
of their current location as long as it is within the available area of the offering operator.
However, this system also implies that the operating organization needs personnel
and equipment to relocate scooters that were discarded in areas with low demand.
Finally, three main models do exist for billing the customers~\cn.
Namely, these are billing per minute, billing per kilometer, or a fixed rate for a whole ride.


\paragraph{German electric scooter manufacturer \textit{e-bility}} \label{sect:e-bility}

The thesis loosely focuses on the business of German electric scooter manufacturer \textit{e-bility}.
The company was established in 2009. For their product, they concentrated on three main aspects:
an exterior design ready for the European market, build quality and sufficient cruising radius.
They finished building their first electric scooter in 2010 and are thriving for good products ever since.
The price of their electric scooters is still relatively high~\cite{e-bility}.
Owing to that circumstance, they came up with smaller, cheaper models
as well as a completely different approach.

Instead of traditional retail sales they started developing the idea of a
short-term rental service for their scooters. And from many angles it makes sense:
As seen in the last section, the market for the short-term rental of electric scooters
especially in Germany is still minuscule compared to e.g. car or bike sharing.
Meanwhile, the costs for buying an electric scooter are still relatively high.
Additionally, many people will not need such a vehicle for the whole day and would benefit
from more flexibility. Lastly, parking spots are also a scarcity in inner cities.
Therefore, \textit{e-bility} plans to enter the market of short-term electric scooter rentals
in the near future. The company plans to sell the scooters as well as a license
to operate a short-term rental service which includes front-end and back-end applications
to interested operators such as municipal works.

There are still many aspects to be discussed and determined but one of them
is already set and can be seen as a necessity regardless of the final implementation:
A back-end application as well as its infrastructure.
The manufacturer \textit{e-bility} acquired an existing software solution
to achieve this although they currently are not in possession of it yet.


\section{Components of a modern software product} \label{sect:frontend/backend}

The main aspect of success for such a system is the \textit{magic} behind the scenes
that no customer sees. People just see the electric scooter as the product and
want to be able to ride one on demand by just pushing a button.
The ease of use for the customers and their satisfaction are the main facets
to be considered by the provider of such a service.

Oftentimes this, as virtually all the short-rental services on the market~\cn,
involves two foundational web-based components --- a front-end and a back-end\cn.
On the one hand, the front-end provides the user interface of the product, the façade of the whole.
It is the component the customer interacts with but more importantly it will be
what the users identify the product with.
On the other hand, the back-end is the brain of the software product.
It provides endpoints for the front-end to use and consume. The back-end dictates
what the front-end will do. Meanwhile, it does not determine what it will look like.
If the front-end is unusable the back-end will never be able to fix that as the
front-end becomes the mental representation of the whole product for the customers.
If the back-end behaves sluggish or is prone to errors, the front-end will not
be able to save much by the same token. In the end, both form a synergy with each other.

This thesis will shed light on the back-end of such an application.
It will focus on the system and software architecture of it.
This includes the functionalities that are mandatory and the question on
how the product might come together --- from ideas and drafts to written source code.


\section{Approaching the development of software} \label{sect:software-development-process}

From a very fundamental standpoint, software development consists of two distinct phases.
The planning and analysis phase which leads into the programming phase~\cite{royce-large-systems}.
Upon further investigation, there are more steps before, in between and after these two essential ones.
In his influential 1970 paper \textit{"Managing the Development of Large Software Systems"},
American computer scientist Winston W. Royce discusses his idea of a satisfactory
software development process. His paper is often mistakenly cited as the starting point
of the \textit{waterfall model} whereas Royce indeed starts his paper with such an approach but quickly
depicts the shortcomings of that model~\cite{royce-large-systems, larman-iid-history}.
The waterfall model describes the idea that a software development process should consist of
the stages shown in figure \ref{fig:waterfall-model}.
\begin{figure}[htbp]
\centering
\includegraphics[width=.60\textwidth]{waterfall-model}
\caption{The essential stages of the waterfall model}
\label{fig:waterfall-model}
\end{figure}
The main point of it is that the steps are conducted sequentially;
no step should begin before the current one has been finished~\cite{boehm-spiral}.
This idea has one big flaw: the \textit{Verification} phase being nearly at the end.
Only after the whole software product is finished, an evaluation by
internal testing personnel or the client can be carried out~\cite{royce-large-systems}.
It is likely that at least some changes to the implementation will be necessary after the \textit{Verification}.
Additionally, clients may not know the exact requirements at the very beginning
of the software development process~\cite{parnas-rational-design-process}.
It is also possible that a major fault is only discovered in that second last stage ---
the negative repercussions could be massive.
\begin{displayquote}
\emph{"Either the requirements must be modified, or a substantial change
in the design is required. In effect the development process has returned to
the origin and one can expect up to a 100-percent overrun
in schedule and/or costs."}~\cite{royce-large-systems}
\end{displayquote}
Nevertheless, the idea Royce describes is what made his paper as influential.
The steps that are required to get from an idea to a finished product nearly always
stay the same. The ones from figure \ref{fig:waterfall-model} are still applicable to
many newer concepts of a software development process.

Nowadays, software developers and project managers are mostly utilizing any model that
can be subsumed under the loose term \textit{agile}.
In summary, as the term already depicts, this theory is geared towards
the idea that the software development process should not be as rigid and constrained.
Therefore, it stands in stark contrast to the demonstrated waterfall model.
The term does not describe an explicit concept but rather an idea ---
there are dozens of implementations of agile software development~\cite{martin-agile-practices}.
Regardless of the implementation, most often agile approaches are based around
the idea of \textit{Iterative and Incremental Development (IID)}~\cite{larman-iid-history}.
\begin{displayquote}
\emph{"Software development should be done incrementally, in stages with
continuous user participation and replanning and
with design-to-cost programming within each stage."}~\cite{mills-iid}
\end{displayquote}
Instead of implementing the whole software within all the sequential steps
of the waterfall model, the product is being developed in increments.
Following some initial analyzing and planning, the first iteration begins.
After the end of one iteration of all the steps shown, apart from the \textit{Maintenance},
the next increment is getting projected and eventually executed as shown in figure \ref{fig:idd-model}.
\begin{figure}[htbp]
\centering
\includegraphics[width=.60\textwidth]{iterative-and-incremental-model}
\caption{A revised, iterative version of the waterfall model i.e. IID}
\label{fig:idd-model}
\end{figure}

To conclude, the stages Royce exhibited in 1970 are still applicable nowadays.
In some form or another, those are mandatory for implementing any software
and became therefore known as the \textit{software development life cycle}.~\cite{se-ieee}
This fact is corroborated by the IEEE's
\textit{Guide to the Software Engineering Body of Knowledge (SWEBOK)}
which is also recognized by the ISO~\cite{swebok}.
This document defines \textit{"knowledge areas"} which match the depicted stages.\\
Consequently, the portrayed stages are discussed in greater detail in the following subsections,
while excluding the last two ones as they will not be part of the
\textit{\nameref{chap:analysis}} chapter.
Moreover, these explanations do not follow an incremental approach
or a sequential one --- the point of view is generalized.


\subsection{Requirements} \label{subsect:requirements}

The \textit{SWEBOK} by IEEE defines \textit{Software Requirements} as:
\begin{displayquote}
\emph{"The Software Requirements knowledge area (KA) is concerned with
the elicitation, analysis, specification, and validation of software requirements
as well as the management of requirements during the whole
life cycle of the software product."}~\cite{swebok}
\end{displayquote}
Generally speaking, two types of requirements do exist --- functional and non-functional ones.
Functional requirements are definitive features of the software whereas non-functional
requirements are describing aspects of the software like performance, security,
user-friendliness and so forth~\cite{sommerville-se}.\\
Furthermore, requirements should be quantifiable; specific key figures should be established
rather than broad statements like \textit{"overall great performance"}.\\
Finally, after specifying the gathered requirements a validation of them should be conducted.
This validation process is not part of this subsection though.


\subsubsection{Elicitation}
Requirements elicitation describes the process of gathering the requirements.
Many stakeholders are part of this process e.g. the customer, software engineers,
available users or a market analysis and the management of the organization conducting
the development.
During the elicitation, proper communication between these parties is the most important
concern. Additionally, the stakeholders should always seek for a set of
requirements that is still extensible later on~\cite{swebok}.

\paragraph{Sources}
The goal or objective of the software should be known and can be used to
shape the overall requirements of the finished product.
Moreover, the engineers and developers should utilize their knowledge especially
toward the customer who usually does not know much about software development.
Doing this, many early requirements by the customer can be rationalized to allow
them to still be included albeit in a reasonable form.
Additionally, all the stakeholders should seek for a balance between them in
terms of their standpoints and requests.\\
Finally, three sources remain that are derived from the organization and its sector.
Namely, these are the organizational, operational, and business environments~\cite{swebok}.

\subparagraph{Organizational environment}
The employees of the organization are oftentimes the ones who use the software
directly or indirectly. Thus, the developed software should fit into the existing
organizational businesses processes. In case a disruption of any business process
is expected due to the new software, this affair should be discussed with the
employees of the organization.

\subparagraph{Operational environment}
The environment where the software will run later is also important for the
aggregation of requirements. If a certain performance is mandatory for the use case
of the software, this should be formed into a concrete requirement.

\subparagraph{Business environment}
Commonly, the business of the organization dictates certain requirements like
payments. For example, for paid services a preeminent requirement is that
no customer should be able to circumvent a payment.


\paragraph{Techniques}
After the sources of the requirements are clarified, certain techniques can be
applied to actually gather them~\cite{sommerville-se, swebok}.\\
Interviews are seen as the most basic form of requirements elicitation;
during these the stakeholders are getting interviewed to garner possible requirements.
Similarly, meetings with all the stakeholders can be conducted to discuss the requirements.
The advantage of this approach is that every stakeholder brings other considerations and
different knowledge into the meeting which allows for better discussions.
Furthermore, prototypes can be created to collect requirements early on while already
receiving feedback for existing ones. Moreover, scenarios and observations can be
conducted. A scenario looks at the use case of the software from the standpoint
of the software engineers while an observation focuses on the perspective of the
users. With the latter technique, it is often possible to discover requirements
that were to minuscule to be mentioned by the other stakeholders or even by the users themselves.


\subsubsection{Analysis}

The analysis and later specification of the requirements begins with the classification
of them and leads into some form of modeling.

\paragraph{Classification}
As already mentioned, the most basic form of classifying requirements is to group
them into functional and non-functional ones. Moreover, it should be discerned
whether a requirement is high-level or low-level in terms of its implementation.
Additionally, the scope of the requirement can be a deciding factor.
If a requirement has major implications on the whole software, it should be classified
differently than a specific but small feature. Similarly, the stakeholders should
estimate how stable a requirement is; if it is likely to change, it should not
play an integral role in the whole system.
Finally, it should be determined how high or low the priority of each requirement is.
This can lead to a more focused design and implementation with regard to achieving
a \textit{minimal viable product (MVP)} sooner than later during
the development process~\cite{swebok, sommerville-se}.

\paragraph{Specification}
After the requirements are classified, they can be modeled into e.g.
a natural language specification or a diagram.
The \textit{SWEBOK} by IEEE mentions \textit{"use case diagrams, data flow models,
state models, goal-based models, user interactions,
object models, data models, and many others."}~\cite{swebok}
The \textit{Unified Modeling Language (UML)} offers good, standardized diagrams
to create these conceptual requirements models.
Moreover, Ian Sommerville states that natural language documents
are still the most widely used form of requirements specification since they are
\textit{expressive, intuitive, and universal}~\cite{sommerville-se}.


\subsection{Design} \label{subsect:design}

Software design is defined as: \textit{"the process of defining
the architecture, components, interfaces, and other characteristics
of a system or component"}~\cite{iso-se}.
The result of said process is the software architecture which describes the
components of the software as well as their interfaces~\cite{swebok}.
However, software architecture has two layers: the high-level design as well as
the low-level design which is about the more detailed
construction of single components~\cite{iso-sdlc}.
Usually, the functional requirements mentioned in the previous subsection dictate
what the low-level architecture will look like. In contrast, non-functional
requirements often determine the high-level architectural design of an application~\cite{bosch-sa}.
However, oftentimes a mixture of both will determine both aspects of the final software architecture.


\subsubsection{Design decisions}

As the word \textit{decision} depicts, \textit{Design decisions} are all about
answering questions regarding the software design. While contemplating about
the answer to each question, the software architecture increasingly emerges.
Additionally, if every decision is made, the architecture automatically gains
sophistication. Ian Sommerville states nine \textit{Design decisions} in his
standard work \textit{Software Engineering}~\cite{sommerville-se}.
For this thesis, four of them were picked to be substantive:
\begin{enumerate}
\item What architectural patterns or styles might be used?
\item What architectural organization is best for delivering the non-functional requirements of the system?
\item What will be the fundamental approach used to structure the system?
\item What strategy will be used to control the operation of the components in the system?
\end{enumerate}
Furthermore, the engineers should think about the following five non-functional characteristics:
\textit{Performance}, \textit{Security}, \textit{Safety}, \textit{Availability},
and \textit{Maintenance}~\cite{sommerville-se}. However, how exactly these are
regarded and achieved is not part of this subsection.


\subsubsection{Principles}

Regardless of the final software architecture, software design principles exist
that are shared and applied in some way by every software; the \textit{SWEBOK} by IEEE
mentions seven of them~\cite{swebok}: \textit{Abstraction}, \textit{Coupling and Cohesion},
\textit{Decomposition and Modularization}, \textit{Information Hiding},
\textit{Separation of Interface and Implementation},
\textit{Sufficiency and Completeness}, and \textit{Separation of Concerns}.
Ideally, if applied correctly, these principles greatly improve the structure
of any software which in return enhance other areas of the
software development process like maintenance. All of the mentioned
software design principles are discussed briefly in the following paragraphs.

\paragraph{Abstraction}
\textit{Abstraction} is about perspective; it tries to achieve that an object or
function only focuses on the information that is mandatory for the execution of
their task while not taking account of the full set of information.
For instance, a function that is used in two locations of an application
should be abstract enough to be used in a third one later on without
any modifications to it.

\paragraph{Coupling and Cohesion}
These two terms are describing how a component behaves in regard to the whole application.
\textit{Cohesion} is a measure of the interdependence of a component whereas
\textit{Coupling} describes how associated the elements of the components are~\cite{swebok}.
Consequently, this concept is therefore close to the aforementioned \textit{Abstraction}
since high \textit{Coupling} lowers the abstraction of a system and vice versa.

\paragraph{Decomposition and Modularization}
Software should be divided into components and even sub-components that each fulfill
an unique task while having concrete interfaces to interact with each other.
Oftentimes, the resulted components are called modules hence the term \textit{Modularization}.

\paragraph{Information Hiding}
From the outside of a component, the internals of its implementation should be hidden.
Thus, data should only be accessed via an interface which in return makes it irrelevant
for other components to know about the processed information inside of a component.
To conclude, this concept is the consequence of the proper application of \textit{Modularization}.

\paragraph{Separation of Interface and Implementation}
Similarly, the interface of a component should not reveal the implementation of
that component. Moreover, this allows for easier maintenance as the implementation
can be changed without modifying much more of the interface than the direct connection
of it to the implementation.

\paragraph{Sufficiency and Completeness}
These terms describe the goal of achieving components that implemented their
planned requirements in an abstract way while not implementing anything else
which is not directly congruent with their purpose.

\paragraph{Separation of Concerns}
A \textit{Concern} is any characteristic of a software system
e.g. the business logic, the user interface, the database, or even distinct features.
All of these should be separated into different portions of the software.
To sum up, all of the mentioned software design principles are the application
of \textit{Separation of Concerns} to some extent~\cite{mitchell-managing-se}.


\subsubsection{Architectural styles} \label{subsubsect:architectural-styles}

Many \textit{Architectural styles}, sometimes called \textit{Architectural patterns},
do exist and can be utilized to model ones software architecture after them.
More precisely, they can be defined as:
\begin{displayquote}
\emph{"Architectural patterns are a means of reusing knowledge about
generic system architectures. They describe the architecture,
explain when it may be used, and discuss its advantages and disadvantages."}~\cite{sommerville-se}
\end{displayquote}
Due to the large amount of such patterns, not every single one can be discussed
in this subsection. Consequently, three patterns were chosen to be discussed
in greater detail in the following paragraphs. Owing to the fact, that this
thesis is focused on web-based client-server systems, as depicted in
section \ref{sect:frontend/backend}, only solutions geared towards a back-end are exhibited.

\paragraph{Layered}
A \textit{Layered} style divides the whole software into distinct layers that
all handle a different task. A famous example is to use a presentation,
business logic, and database layer. However, an application can have completely different
layers. This concept most likely demands a \textit{monolithic} deployment approach;
that means an interwoven application running as a single program~\cn.
In theory, this approach is fairly testable and maintainable
as the layers can be seen and extended separately. Additionally, this approach demands
for no middleware as described in subsection \ref{subsect:construction} which
makes it easy to get started with for many different types of software solutions.

Nevertheless, this concept has some severe shortcomings.
A developer typically needs to grasp all the layers to understand the whole application.
Moreover, it a difficult task to draw a clear line where one layer ends and the
next one begins.
Furthermore, even if the engineers clearly stated that border,
it is very easy to skip over certain layers by e.g. implementing some logic
in the interfaces towards the presentational layer whereas logic should only occur
inside of the business logic layer; this increases the \textit{Coupling} of the application.
Finally, the application unavoidably becomes larger and slower to a point
where it starts not to perform ideally anymore.
Additionally, at this point, it can become difficult to maintain the application due
to its size and/or possibly higher \textit{Coupling}~\cite{richards-sa-patterns}.
The result is what many engineers contemptuously denominate
as a \textit{big ball of mud}~\cite{fairbanks-sa}.

\paragraph{Event-driven}
This architectural style is often called \textit{publish-subscribe style} as well~\cite{fairbanks-sa}.
This concept works by having separated components that each fulfill certain tasks.
In between all of the components is an \textit{event bus} that accepts events from
the components and makes them available to all the other ones. Thus, this approach
demands for some middleware, the \textit{event bus} at least.
Components can subscribe to an event type while they and the others publish events
that occurred during their execution. If an event gets published that another component
is subscribed to, it can pick that event up via the \textit{event bus} and invoke
a pre-defined action. That way, the whole application becomes implicitly invoked;
no component directly calls functions of another one~\cite{garlan-shaw-sa}.
In consequence, this concept has some very unique advantages as well as disadvantages.
The \textit{Coupling} of an application following the \textit{Event-driven} approach
is lower since the subscribing components do not even know which other component
published the event. Additionally, proper \textit{Modularization} and \textit{Information Hiding}
can be achieved more easily. Therefore, the maintainability as well as the extensibility
of the application are significantly better; publishing components can easily be replaced
as long as the published events stay the same.

A downside is the fact that opposed to traditional call-based architectures,
the components of an \textit{Event-driven} architecture receive no feedback regarding
their published events. Especially if a subscribing component publishes a new event
in response to the received one; the component of the originally published
event never comes to know if its event was successfully consumed or not ---
it becomes oblivious.
Additionally, the visualization of such an \textit{Event-driven} approach
is drastically more difficult. Due to the fact that every component can
\textit{"talk"} to every other one and that the \textit{event bus} is just in between
all of them, the resulting diagram can become noticeably ambiguous~\cite{fairbanks-sa}.
Finally, error handling can become a tedious task since an error in one component
often means that another component has to rollback their preceding execution which
triggered the published event. A rollback strategy has to be conceptualized to
achieve appropriate error handling~\cite{richards-sa-patterns}.

\paragraph{Service-oriented}
A \textit{Service-oriented} approach divides the whole application into distinct
services that all run as separate programs. Thus, this style is the
polar opposite of the aforementioned \textit{monolith}.
This architectural style includes the \textit{Microservice} architecture
which is getting popular in recent years. The main distinction is that
\textit{Microservices} are generally more fine-grained although the definition
is rather ambiguous depending on the source.
The single services call each other via
\textit{remote procedure calls (RPC)} or even over the internet utilizing HTTP
and a data exchange format such as JSON~\cn.
The advantages are clear compared to a \textit{monolithic} approach.
The services can be fully replaced if their interfaces stay the same and the
scaling of single services becomes easier since every one of them runs as a single program.
Moreover, the reusability of services can be better if each service is properly \textit{abstracted}.
Furthermore, a \textit{Service-oriented} approach even allows for multiple programming languages
across different services. Albeit the complexity increases, it becomes possible
to choose \textit{the best tool for the task}.
Another advantage is that the single services can be duplicated independently
in case some of them experience more load than others.

The main difficulty arises from the fact that it is oftentimes problematic to cleanly
divide a business logic into distinct services~\cite{fowler-monolith-first}.
Additionally, the performance of the application is oftentimes worse compared to
a \textit{monolith} one since the interface interactions are conducted across multiple
executed programs~\cite{richards-sa-patterns}.
Finally, compared to e.g. a layered approach there is much more setup needed.
The orchestration of the services requires middleware (see \ref{subsect:construction})
to ensure the flawless execution of the application~\cn.

The most important middleware is a \textit{Service Discovery} which keeps track
of the addresses of all the available services so the integration of them into
each other becomes faster, easier and less error-prone.
At some point, this can be paired with a load balancer in case
multiple instances of some services do exist.
Furthermore, a gateway into the service infrastructure is mandatory to guard
them from security threats from outside. Especially an API gateway can be
utilized to automatically route incoming requests from external users into the
application without letting the public access the services directly.\\

In summary, all of the portrayed styles dramatically change the way the final product
is structured. Thus, the step of choosing an architectural style is an, if not the
most important part of the software design process. After choosing a style,
it is only useful to visualize the components of the software utilizing the chosen style.


\subsubsection{Visualization}

There are two general types of notations regarding software architectures~\cite{sommerville-se}.
The simpler one is the \textit{block diagram}; it portrays the components of
a software as well as their sub-components. Additionally, directed arrows depict
the data flow between them. However, \textit{block diagrams} are neither a
standardized notation nor do they illustrate much detail.
Therefore, a \textit{block diagram} can be used in the beginning of the software
design process to have an abstract of the overall software architecture.
Moreover, this visualization technique can be used to get everybody on the development team
on the same level. Owing to their relative simplicity, even non-developer members
of the team can grasp the designed architecture. The final result of
the software design process can be a more standardized visualization
e.g. a type of UML diagram. However, this approach is oftentimes already too close
to the implementation which favors a \textit{block diagram} as the general
approach to get started with~\cite{sommerville-se}. A more detailed and standardized
visualization is however useful for a thorough documentation.


\subsection{Construction} \label{subsect:construction}

Some important concepts are at the core of implementing software.
The engineers and developers should try to minimize the complexity of the whole system
to make it more maintainable in the future.
Additionally, source code reuse should be administered either with own source code,
external libraries or even COTS~\cite{swebok}.
Furthermore, the development team should agree upon a set of standards.
This starts with obvious choices like the programming languages of the software
and continues with coding standards and tooling~\cite{mcconnell-code-complete}.
Finally, testing and integration are another set of considerations.
Unit testing is an important tool to continuously test the developed components;
it describes the procedure of testing single \textit{units} respectively components
of the codebase with comparing a known set of correctly corresponding input and output
with the actual output with the given input~\cn.
Moreover, after finishing the construction of the software or even during that stage,
an integration strategy has to be deployed. Namely, these are either a
\textit{phased} or an \textit{incremental} approach. A \textit{phased} integration
process waits for all the necessary components to finish and assembles them into a
holistic solution in one step. In contrast, an \textit{incremental} approach
integrates components once they are finished. These two general approaches
were already reflected in section \ref{sect:software-development-process}.

\subsubsection{Considerations}
The actual construction can be conducted in many different ways.
The \textit{SWEBOK} by IEEE mentions sixteen distinct considerations~\cite{swebok}.
The most important ones for this thesis are discussed in the following paragraphs.

\paragraph{Assertions and \textit{Defensive Programming}}
The input parameters of a function can be checked to ensure that their types
or general format are correct. This can eliminate many runtime issues while not
being overly difficult to employ. Similarly, the term \textit{Defensive Programming}
refers to the procedure of meticulously checking every input value of a function
to prevent its breakup. Additionally, a strategy needs to be deployed on how to
handle corrupted inputs~\cite{swebok}.

\paragraph{Error handling}
It is customary that software spawns errors. Consequently, a strategy on how
to handle errors should be deployed. This can include returning the error code
and/or error message to the calling routine, logging the error message, or
eventually to halt the execution of the software~\cite{swebok}.\\
The most well-known and most common approach however is \textit{exception handling}.
This concept uses \textit{try/catch-blocks} to execute code inside of
the \textit{try-segment}. In case of an exceptional error event,
an \textit{exception} is thrown. Further on, the \textit{catch-segment}
implements a strategy on how to handle the occurred \textit{exception};
this can be one of the aforementioned ones, for example.

\paragraph{API design}
If a software is comprised of externally available interfaces,
one or multiple Application Programming Interfaces (APIs) should be developed
on top of the actual components. An API exposes the software to external programs
and functions like an interface. Its distinctiveness stems from the fact that
an API should be hard to misuse, easy to extend, stable, and backwards compatible~\cite{swebok}.

\paragraph{Runtime configuration}
Oftentimes, the corporate users of a finished software solution want to be able
to change the behavior of the software by supplying different configurations.
Traditionally, this is accomplished by using configuration files which allow
the users to change certain characteristics by choosing from a set of options.
With proper \textit{Assertions} of the input configuration, this feature of a
software is a good approach to allow non-developers to change the deployed software
in a fenced manner.

\paragraph{Concurrency}
Nowadays, software processes can be executed concurrently to maximize the performance
of said process. In case the engineers and developers of a software solution
are utilizing concurrency models, it is important to agree on a strategy to
mitigate runtime problems arising from running a process concurrently.

\paragraph{Middleware}
The term \textit{middleware} is very broad. In essence, it describes a piece
of software that is not part of an operating system whose purpose is to
offer easier communication between other software solutions e.g. by handling
their input and output in a standardized way.
Middleware is often utilized in a distributed or service-oriented system architecture,
owing to the fact that connecting the services is a task that every developer
of such software has to undergo which brings forth standardizations.


\section{Legacy software} \label{sect:legacy-software}

The term \textit{legacy software} describes a software product that was created
some time in the past and thus became outdated in terms of the utilized technology
stack, its performance and security or its software architecture~\cite{seacord-modernizing-legacy}.
All of the above can occur at once or emerge partially.\\
The term often bears a pejorative association with it although legacy software
is indispensable for the organization that makes use of it. Most of the time, it
still fulfills the needs of its users and is therefore still profitable~\cite{bennett-coping-legacy}.
Moreover, the source code sometimes comprises the knowledge of the organization while
the software itself is archaic. Owing to that fact, discontinuing legacy software can be difficult.
In the end, there are two general options: a complete replacement of the legacy system or
a partial extension of it.


\subsection{Reasons to continue legacy software} \label{sub-sect:continue-legacy}

Formerly, a main reason to keep a software solution running was tied to its
hardware requirements. Legacy software that is many decades old might still
run on mainframe computers opposed to contemporary server systems~\cite{schneidewind-preserve-or-redesign}
or even cloud computing. However, these considerations are not part of this thesis
since the analyzed software solution was built as a common client-server application
meant to run on prevailing server infrastructure.

Apart from this initial reason, the most common one is of economic nature~\cite{schneidewind-preserve-or-redesign}.
Often times, the expected benefits of redeveloping a software cannot justify the expenditures.
Additionally, it is difficult to measure the advantages beforehand. The management
of a company needs to ponder the proposal of redeveloping the current software.
Particularly, if the legacy software still seems to be running as expected when observed
from the outside, is becomes harder for the developers and engineers to vindicate
their proposal towards the management --- raw characteristics such as performance,
security, maintainability and eventually the development time can just be roughly estimated and
are bound to the experience of the developers and engineers in charge.
Moreover, even if the analysis does sound promising there is no guarantee that the new system
will meet the expectations. Nevertheless, the tipping point can occur when the human costs for the maintenance
of the legacy software are tremendous when compared to development teams of similar size and complexity.

Furthermore, required constant availability or faultlessness of a system might make it
difficult or impossible to implement a new software solution for its purpose~\cn ---
software that controls a nuclear power plant comes to mind, for instance.
Economic considerations offer an additional point of view:
\begin{displayquote}
\emph{"If a legacy system is running the key billing system, it is not sensible
to make rash judgments, because the very future of the business may be at stake."}~\cite{bennett-coping-legacy}
\end{displayquote}
Especially the testing of a critical application might increase the costs
and/or risks significantly. These aspects might make it improbable to replace
the legacy software. Owing to the fact that the legacy software still fulfills
its purpose, much effort is needed to ensure that the replacement software works
in the same expected and quantifiable manner as its legacy counterpart.

In addition, it is also possible that the users of the legacy software do not want
to switch to a new solution due to third-party vendor lock-in or the fact that
they are familiar with the legacy system~\cite{bennett-coping-legacy} which would make
retraining a vast cost driver.

Lastly, a big concern that the company's management cannot even control is
poor or missing documentation.~\cn This circumstance is the most difficult one
as it makes the redevelopment or the extension of the legacy software very intricate.
Consequently, this can go as far to leave the organization unable to completely apprehend
every detail of its legacy software.

In summary, these four aspects are mostly accountable for the continuation of legacy software
within an organization. Albeit, the assessment of costs versus benefits
can be seen as the most important one by far~\cite{schneidewind-preserve-or-redesign}.


\subsection{Issues with legacy software} \label{issues-legacy-software}

The last-named reason of a poor documentation in favor of continuing a
legacy system is likewise a major issue~\cite{bisbal-legacy-issues}.
The developers know far less about potential side-effects and thus
fixing a bug or refactoring some source code becomes a tedious task.
In consequence, human expenditures escalate compared to a well-known and well-documented
solution owing to the fact that any activity on the legacy system takes more time.
After all, the maintenance of poorly documented legacy software
is more time-consuming and thus more costly.

The second issue emerges when a legacy software solution receives many
patches and general improvements over time; its maintenance
turns into a problem. The system becomes overly brittle due to its
increasing complexity~\cite{seacord-modernizing-legacy, tilley-perspectives-reengineering}.
This brittleness engenders a growing number of side-effects and bugs
to the point where further extending the software becomes an ordeal.

The final issue is about the absence of proper interfaces
in legacy systems~\cite{bisbal-legacy-issues}. This problem manifests itself twofold.
Firstly, integrating a legacy system into another system is onerous.
Legacy software often completely lacks an externally accessible API.
Therefore, connecting an aged technology stack of a legacy system to a completely
different type of architectural design, most often running on a different programming language,
might be virtually impossible or only achievable with severe wrapping
of the interfaces of the legacy system. However, that intensifies the aforementioned
issue of high human costs all the more.
Secondly, the extensibility of a legacy system is worse.
The developers that once created the legacy software might not be part of the
organization anymore. If this fact is paired with poor documentation and/or
an inferior software architecture, extending such a legacy solution is once again
very time-consuming. For example, if a legacy software features
low cohesion but high coupling, the converse way of how it should be~\cn,
extending it might introduce unwanted side-effects which increase debugging
time by a large margin.


\subsection{Solutions to legacy software issues} \label{solutions-issues-legacy-software}

Although, as just mentioned, the extensibility of legacy software might be mediocre,
a couple of concepts do exist to overhaul an existing legacy software solution.
Generally speaking, they range from an evolution to a revolution~\cite{bisbal-legacy-issues}.
The approach of \textit{Maintenance} is excluded from this subsection as it
has to be applied to every software product as shown in section \ref{sect:software-development-process}.
Nevertheless, it is a legitimate approach to cope with legacy software.
In consequence, four main concepts can be defined as solutions to legacy software issues ---
they are displayed in the following figure \ref{fig:coping-legacy}.
The axis depicts the amount of changes each approach entails.
However, it is important to discern that it is likely that a combination of
the explained concepts is utilized~\cite{bisbal-legacy-issues}.
Some parts of a legacy system might be wrapped while others need to be redeveloped.
\begin{figure}[htbp]
\centering
\includegraphics[width=.80\textwidth]{coping-with-legacy-systems}
\caption{Four approaches to cope with legacy software systems~\cite{bisbal-legacy-issues}}
\label{fig:coping-legacy}
\end{figure}


\subsubsection{Redevelopment}

To start from the far right, \textit{Redevelopment} is the most disruptive approach;
it implicates the discontinuation of the currently used software.
While having the huge disadvantage of being immensely cost-intensive, it indisputably
yields the best results opposed to the issues with legacy software presented in
subsection \ref{issues-legacy-software}.
Nevertheless, the second big risk apart from economic concerns is
the risk of failure.
Finally, depending on the size of the project, it is possible that at the end
of a long development cycle for the new software solution, it becomes outdated
yet again by not fully meeting the ever-changing
business needs anymore~\cite{stevens-software-reengineering-patterns}.


\subsubsection{Migration}

A median concept on the scale of figure \ref{fig:coping-legacy} is \textit{Migration}
which is also known as \textit{Reengineering}~\cite{tilley-perspectives-reengineering}.
These hypernyms actually describe two different solutions in terms of their goals;
they are, however, often used interchangeably~\cite{bisbal-legacy-issues}.
While both terms imply the extension of a legacy system, \textit{Reengineering}
has the goal to phase out the existing system eventually.
On the other hand, \textit{Migration} is just about augmenting the currently used
solution with new functional components while replacing some others that cannot be used anymore
for any reason. To put it into the words of Jesús Bisbal et al.:
\begin{displayquote}
\emph{"Migration seeks to reuse as much of the legacy information system as possible,
including implementation, design, specification, and requirements."}~\cite{bisbal-legacy-issues}
\end{displayquote}
Meanwhile, Scott R. Tilley and Dennis Smith define \textit{Reengineering} as:
\begin{displayquote}
\emph{"Reengineering is the systematic transformation of an existing system
into a new form […]"}~\cite{tilley-perspectives-reengineering}
\end{displayquote}
Consequently, these definitions put \textit{Reengineering} closer to \textit{Redevelopment},
due to the fact that both pursue the same long-term goal.
Nevertheless, \textit{Reengineering} is a type of \textit{Migration} and
therefore conducted similarly.\\
Concretely, there are four major approaches to a \textit{Migration}
process~\cite{malinova-legacy-techniques, seacord-modernizing-legacy},
ranging from a moderate one to the just mentioned \textit{Reengineering}
which is oftentimes a larger proposition.

\paragraph{Retargeting}
If for any reason the hardware or utilized third-party software got outdated,
the legacy software can be migrated to more performant hardware or
up-to-date versions of the leveraged third-party software.\\
\textit{Example}: Migrating the existing legacy software from traditional server
hardware to cloud-computing.

\paragraph{Conversion}
However, the changes in hardware or third-party software might be too disruptive
to perform them directly. In that case, a \textit{Conversion} of utilized
programming languages or technology stacks can be conducted.
Of course, this process is accompanied by much more work.\\
\textit{Example}: Converting the existing non-relational database to a
new database management system (DBMS) which is based on the relational SQL.

\paragraph{COTS components}
Furthermore, \textit{commercial-off-the-shelf (COTS) components} can be
leveraged to replace certain parts of a legacy software which oftentimes also
augments it, owing to the fact that such products are usually offering a wide
variety of features.\\
\textit{Example}: Replacing the customer management component of an existing
legacy software with a current COTS \textit{customer relationship management (CRM)} solution.

\paragraph{Reengineering}
As already mentioned, this last approach is the most labor-intensive yet most promising one.
During this procedure, the codebase gets refactored or reengineered
into completely new components which then replace outdated ones.
However, this process happens over time opposed to a \textit{Redevelopment} approach.\\
\textit{Example}: Transfer an increasing amount of distinct components
of the legacy application into web services.


\subsubsection{Wrapping}

To conclude, \textit{Wrapping} is the least revolutionary process.
In fact, it actively endorses the continuation of a legacy software solution as
portrayed in subsection \ref{sub-sect:continue-legacy}.
This concept involves wrapping the existing codebase into an isolated package
which is then just accessed via a newly created interface e.g. an API.
The existing legacy software receives input parameters and returns an output.
The substantial problem with this whole concept is that the legacy software does get
extended only unidirectionally. It cannot invoke calls towards the newly added components.
Therefore, this solution is rather short-term in nature.
This concept can be divided into four disparate levels~\cite{sneed-encapsulating-legacy}.

\paragraph{Database}
A \textit{database wrapper} is the least interfering approach due to the fact that
the existing source code can stay as it is; it just exposes the database of the current system
to the outside.\\
\textit{Example}: The database table persisting the print jobs of an existing legacy software
is available for external programs.

\paragraph{Service}
The \textit{service wrapper} encapsulates distinct services from the legacy software
into callable entities which can thus be invoked from an external system.\\
\textit{Example}: The entire reservation system of an existing legacy software
can be called via an API by an external system.

\paragraph{Application}
An \textit{application wrapper} provides single classes or whole components for external usage.
It is similar to a \textit{service wrapper}, however, the wrapping is applied on a more
compartmentalized level.\\
\textit{Example}: The object of a class to create print jobs in a certain format
gets exposed to the outside, allowing it to be used by external programs.

\paragraph{Function}
Finally, \textit{function wrappers} attach to the most microscopic portions of a software ---
its functions. This approach can be useful if both low cohesion and low coupling are prevalent
in the present legacy software.\\
\textit{Example}: A single function that calculates a specific taxation value
can be invoked by an external process.
\newline

To sum up, Robert C. Seacord also mentions \textit{Black-Box Modernization} as well as
\textit{White-Box Modernization}~\cite{seacord-modernizing-legacy} which are
effectively \textit{Wrapping} and \textit{Migration}. To put this into words,
if an organization conducts the \textit{Wrapping} approach, the main goal is not
to handle the internals of the legacy software.
Meanwhile, a \textit{Migration} process embraces subtle changes to the codebase of the software.\\
In consequence, owing to the fact that most of the time \textit{Wrapping} still
involves some changes to the codebase, \textit{Migration} can be a better
approach since it results in a longer-term solution.\\
Nevertheless, the primary focus of both approaches is to achieve a greater
return on investment (ROI) compared to a \textit{Redevelopment}~\cite{tilley-perspectives-reengineering}.



\chapter{Analysis} \label{chap:analysis}

\epigraphhead[55]{\epigraph{Once a system becomes a ball of mud,
some developers find security and prestige in being the select few who
can understand and evolve it, while those who detest the mud […] run away.
The result is that the ball of mud is rarely cleaned up.}
{\textit{George H. Fairbanks}}}

This chapter is structured similarly to the \textit{\nameref{chap:fundamentals}}
chapter. The first part is about the system and software architecture of
a back-end application for the short-term rental of electric scooters.
Consequently, the stages of a software development process as depicted
in section \ref{sect:software-development-process} are being exhibited.


\section{Requirements} \label{subsect:app-requirements}

As stated in subsection \ref{subsect:requirements}, the first step of
requirements engineering is the \textit{Elicitation}.
For that purpose, three executive stakeholders
at \textit{e-bility}, the corporate partner of this thesis, were interviewed
to obtain the requirements for an ideal back-end application for the
short-term rental of electric scooters.\\
The discussed software solution is meant for being licensed to
the actual operators of such a rental business. Therefore, an operator
is expected to maintain the application. Additionally, according to the
mentioned \textit{Environments}, three first requirements emerge:

\paragraph{Organizational}
The operating organization will employ service workers who
control certain aspects of the software. They are therefore still users of the
software albeit not being the target group. At this point it is still unclear
how much personnel would be sufficient but a small team of service workers is
enough to execute the business process and to serve the paying users.

\paragraph{Operational}
The service workers will use the non-public front-end of the software within a
web browser. Therefore, the interfaces of the software should be usable by
a web-based front-end solution.

\paragraph{Business}
The whole business model will be based around paying users. Consequently,
a working automated billing process as well as arrears billing is absolutely
crucial. It is expected to leverage the services of a third-party payment
provider owing to the fact that the operating organizations do not want to
handle private credit card information and the likes.


\subsubsection{Use Case}

The interview with the stakeholders quickly resolved around the use case
of such an application which yielded many requirements:\\

The user will have to download a mobile application or use the web app
of the operating organization. Consequently, both types of front-end applications
should be able to use the back-end application.\\\\
No user should be able to do anything without being logged into their account.
Thus, an implemented enrollment process is mandatory.\\\\
Within that process the user will have to fill in his or her
contact information, a valid driver's license as well as billing information.\\\\
Finally, the customer has to accept the terms and conditions of the operating
organization as well as the chosen billing model as described in
section \ref{sect:electric-scooter-market}. The operator should be able to chose
their billing model for all the customers application-wide.\\\\
After successful enrollment, the user will be greeted with a map of the current
location. This map should display all the currently available scooters at
their respective locations. The user can then browse the selection
of scooters and eventually chose one to rent.\\\\
The rental process will begin with a reservation of the chosen scooter for
an arbitrary time that the operator should be able to change application-wide.\\\\
Consequently, during the reservation time and the actual rental period,
the scooter should not appear on the map anymore for other users.\\\\
Next, the user is granted the set timeframe to get to the chosen scooter.
If the user arrives to late, the scooter should re-appear on the map,
available to every customer.\\\\
Once he or she arrived, the scooter as well as its ignition can be remotely
unlocked by the press of a button inside of the mobile app.
The back-end application however actually sends the signal to the scooter to
unlock itself.\\\\
Afterwards, the user can begin the ride for a previously chosen timeframe
or even indefinitely. After being finished, the user can discard of the scooter
anywhere within a certain range set by the operator.\\\\
The operator also needs to see a map with every scooter regardless of battery
charge or availability, mainly to plan out the relocation of certain ones.\\\\
Finally, the scooter is getting locked again and will re-appear on the map for the other users.\\\\
Moreover, during the ride, the scooter constantly sends its location as well
as other information about the current speed and battery statistics.
These information will be sent to a separate server which just persists them
without processing them any further.
Consequently, the back-end application will regularly fetch the information data
from said external server to monitor every ongoing ride of all the customers.\\\\
If a user drives out of the allowed operating range, a notification will be sent
to the scooter of that user.\\\\
If the battery gets low on charge, the user will be warned as well to finish the ride.\\\\
Additionally, after said ride has ended the scooter should also not re-appear
on the map of available ones.\\\\
The threshold for the low battery condition should also be changeable
application-wide by the operator.\\\\
Finally, the billing process should begin after the ride.\\\\
Furthermore, the user should be able to get statistics about his or her rides
e.g. the driven distance, the costs, the average speed and theoretically
many more. Therefore, the back-end application should allow those information
to be accessed by each customer while not exposing the statistics of others.\\

These are all of the functional requirements;
Figure \ref{fig:use-case-diagram} is used to put them into perspective.\\

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{use-case-diagram}
\caption{UML use case diagram based on the requirements}
\label{fig:use-case-diagram}
\end{figure}

Additionally, there is just one non-functional requirement.
As depicted in section \ref{sect:electric-scooter-market}, the current market
for the short-term rental of electric scooters is still fairly small.
Consequently, it is difficult to estimate how many users will use the system
in future years. Thus, the only non-functional requirement is that the system
should be easily scalable in case of a significant influx of new customers.


\section{Design}

The fundamentals of the software design are determined by answering the four
\textit{design decisions} posed in subsection \ref{subsect:design}.

\paragraph{What architectural patterns or styles might be used?}
A layered, monolithic approach poses the problem that it is too difficult
to integrate into an existing solution. In the end the old and the new
application would feasibly just communicate via API calls which would result
in two completely separate solutions. Furthermore, it is tedious to discern
which tasks get fulfilled by the old solution and which ones by the new one.
As a result, a vast interface in front of them both would be needed;
however that is an undesirable state for any application.

The system could follow an event-driven approach owing to the fact that
many of the depicted components of the use case have to run constantly without
much user interaction. Thus, implicitly invoked actions within the application
are feasible. Especially the \textit{Monitoring} component could benefit from
just publishing events while other components subscribe to them.
However, since an event-driven approach would require all the
functional components to work with events, the changes to the legacy solution
would be vast. For instance, while letting legacy components publish events
is still feasible, refactoring every possible interface to be able
to subscribe to events involves tremendous refactoring activities.

In contrast, the system could be structured in a service-oriented matter with
every larger component being a separate one. Other ones can be combined into
one service; however such decisions have to be made much more fine-grained.
The main advantage of the such an approach would be that it is easier to
integrate into an existing system compared to the other two approaches.
The modernization and extension of the legacy solution can be started with
re-developing a small component of the existing solution as a service.
Afterwards, the new service can be integrated into the legacy solution
in place of the old component.

In consequence, a service-oriented approach would best fit the depicted use case
while offering the possibility to iteratively extend and replace portions of
an existing legacy software solution with newly developed services.

\paragraph{What architectural organization is best for delivering the non-functional requirements of the system?}
Scaling is especially easy with a service-oriented approach. The single services
can be scaled up and down individually instead of scaling the whole application.
Some services will probably never see much load as they are not correlating with
increasing user counts whereas other services perform computation-intensive
actions as the monitoring of the scooter fleet; with a service-oriented approach,
the services in high demand can be scaled and even duplicated while others
remain as a single instance.

\paragraph{What will be the fundamental approach used to structure the system?}
A service-oriented approach already dictates the way an application is structured.
Each distinct group of tasks should be modeled into a separate service.
Even components that fulfill just a small task should exist as distinct services.
In summary, the functional requirements, and especially the components of
the use case diagram, should be separated into services based on their tasks.

\paragraph{What strategy will be used to control the operation of the components in the system?}
As stated in subsection \ref{subsect:design}, a service-oriented approach
requires much more middleware than e.g. a layered architecture.
The single services will be controlled and orchestrated via the middleware components
exhibited in the subsection on \textit{\nameref{subsubsect:architectural-styles}}.


\subsubsection{Architecture}

To model the software architecture, all of the components are getting divided
into distinct services based on their tasks. Consequently, many of the
\textit{Principles} stated in subsection \ref{subsect:design} are already fulfilled
partially or even completely. Good \textit{Decomposition and Modularization} is
easy to achieve in a service-oriented architecture --- the single services
are a direct application of \textit{Decomposition}.
Moreover, \textit{Information Hiding} is also a given fact when using a
service-oriented architecture.
Additionally, owing to the fact that a separate front-end will utilize the API
of the back-end, the interface and the implementation will be completely separated.
This statement is even corroborated by the fact that there will be three front-ends,
a mobile app as well as a web-based one for the customers additionally to the
web-based front-end for the service workers of the operating organization.
Finally, the shown services are modeled to feature a maximum of \textit{Abstraction}
as well as low \textit{Coupling}. This is achieved by splitting up components
into smaller services, should their tasks be too fine-grained without being
bound too much to them.

Based on these assumptions and the answered \textit{design decisions},
a \textit{block diagram} is utilized to portray the software architecture of the
designed application (figure \ref{fig:service-architecture}).\\

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{service-architecture}
\caption{The services and their intercommunication}
\label{fig:service-architecture}
\end{figure}

The diagram depicts the single services of the designed software architecture.
The arrows in between them denote the directions of HTTP requests as well
as the allowed HTTP methods for each request.
This can be an important indicator to conceive which services
will only be hit by \textit{GET requests} which are by definition \textit{safe},
in the way that they will never mutate any data~\cite{http-rfc}.
For example, the \textit{Telemetry Data Retrieval} service will never modify
its underlying data upon requests from other services.

Each of them implements their own API that provides the functionalities that
should be available for the other services.

The \textit{API Gateway} as well as the \textit{Scooter Gateway} are described
in greater detail in subsection \ref{subsect:analysis-construction}.

As already portrayed in the use case diagram (figure \ref{fig:use-case-diagram}),
the users of the back-end application will consist of service workers employed
by the operating organization as well as the customers.
The authorization towards the \textit{API Gateway} will determine which
API endpoints customers can access and which ones are just available to
service workers. For instance, the locations and information of all the scooters
of the fleet will just be available to authorized service workers via the
\textit{Fleet Monitor} service.

The \textit{Telemetry Data Retrieval} service fetches current data from the
off-site \textit{Telemetry Server} in periodical intervals.
Consequently, there will be a firewall in between these components.
This data consists of information such as the locations of all the scooters
of the fleet, their remaining battery power, as well as other information.
These information are crucial for the whole application.
Every scooter sends said information to the \textit{Telemetry Server}
periodically as well.

There should be two caches in the architecture to take away load from the
adjacent services. First, the aforementioned \textit{Telemetry Data Retrieval}
service will save new data into the cache which will automatically clear old
data after a preset amount of time. Thereby, nearly all of the requests from
other services apart from the \textit{Statistics Retrieval} one will hit the
\textit{Telemetry Data Cache} as those are only working with current data due
to their monitoring nature.\\
Second, the \textit{Available Scooter Cache} exists which provides the locations
of all the currently available scooters to customers that are currently looking
for a scooter to rent. Owing to the fact that this data will be requested by
many users very often, a cache is a good way to take away that potential load
from the \textit{Fleet Monitor} service which is already the one with
presumably the heaviest load.

The \textit{Driver's license check-up} as well as the \textit{Billing} service
will be closely related to third-party providers of such tasks.\\

Additionally to the use case diagram, a swimlane diagram
(figure \ref{fig:swimlane-rental-process}) has been created to depict
the flow of information between the customer, the scooter and the
involved services during the process of successfully renting a scooter.
This use case is by far the most important one which distinguishes itself
from other common use cases e.g. registering an account.
The two gateways were purposely omitted from the diagram as their tasks would
be the same ones over and over again.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{swimlane-rental-process}
\caption{Swimlane diagram portraying the rental process}
\label{fig:swimlane-rental-process}
\end{figure}

The oval shapes represent user interactions whereas the square shapes represent
machine actions.

The solid horizontal line in the middle depicts the customer riding the scooter
which can take an indeterminate amount of time while no information is flowing.


\section{Construction} \label{subsect:analysis-construction}

\paragraph{COTS components}
As recently mentioned, the \textit{Driver's license check-up} and \textit{Billing}
services will leverage COTS software to verify uploaded driver's license scans
and to handle the billing of customers that successfully rented a scooter.
Consequently, said services will implement just the interfaces towards these
third-party solution. Additionally, the \textit{Billing} service might have
to implement some data persistence logic for accounting reasons.

\paragraph{Utilized programming languages}
As already stated in the subsection on \textit{\nameref{subsubsect:architectural-styles}},
theoretically multiple programming languages across services could be utilized.
For this architecture however, the programming language \textit{Go} has been chosen.
The main rationales for that choice are the following.

The language is among the fastest regarding its execution performance~\cn.
Furthermore, \textit{Go} is a compiled language running directly as an executable
without a virtual machine. Additionally, all the imports are statically linked.
Thereby, the resulting executable is a single file with all the requirements
included and without the need of an interpreter or any other runtime environment.
As a consequence, the deployment of a \textit{Go} program is extremely easy as
there are no prerequisites for the server that executes it.
Finally, \textit{Go} offers a built-in HTTP server which can directly be used
for the intercommunication of the services.

\paragraph{Utilized libraries}
Albeit the HTTP server of the \textit{Go} language being very powerful,
the library \textit{Go Micro} was chosen for scaffolding the services.
It immensely foreshortens the time it takes to setup a service.
Thereby, the developers can focus just on implementing the business logic
of each service while not tinkering around with the scaffold of the service.
\textit{Go Micro} offers methods to quickly generate said scaffold on top
of implemented business logic by mapping business logic functions to endpoints.
It works with RPC and/or \textit{JSON over HTTP} which are both discussed
in detail in a following paragraph on \textit{\nameref{construction-api_design}}.
Additionally, \textit{Go Micro} offers much more optional functionality;
the most interesting one for integrating services into a legacy software solution
is that \textit{Go Micro} can automatically expose the endpoints of a service
as a normal HTTP API. Thus, every programming language with an HTTP client,
which is nearly every single one, can access, utilize, and consume the services
functionalities without other prerequisites~\cite{go-micro}.


\subsubsection{Considerations}

\paragraph{Middleware}
As stated in subsection \ref{subsect:design}, two main middleware components
are mandatory for running a service-oriented architecture ---
a \textit{Service Discovery} as well as an \textit{API Gateway}.
A load balancer is part of such an architecture in case services are mirrored
for scaling purposes. Owning to the fact, that currently it is not foreseeable
how many users such an application would have, this middleware is not part of
this subsection.

\subparagraph{Service Discovery} \label{subparag:service-discovery}
For the \textit{Service Discovery} middleware, an existing solution will be
utilized since it is generally not part of a service-oriented architecture
to developed the \textit{Service Discovery} by oneself~\cn.
For this purpose, the freeware tool \textit{Consul} developed by \textit{HashiCorp}
was chosen to serve as the \textit{Service Discovery} middleware~\cite{consul}.
The core functionality of \textit{Consul} is similar to the mode of operation
of every \textit{Service Discovery} middleware.\\
The main functionality is that \textit{Consul} provides a DNS server which is
made available on the network. It can even work beyond multiple data centers
but since scaling is not directly part of this thesis, this feature is not utilized.
Every service in the architecture can now make a DNS lookup towards the so-called
\textit{Consul agent} which is the main server process of the tool.
This \textit{Consul agent} then returns an IP address and a
port number of the requested service. Additionally, it offers simple
load balancing by randomizing which instance of the requested service
i.e. which combination of IP address and port number is getting returned.
An example lookup is structured like this: \textit{servicename.service.consul};
thereby a service can just lookup this address and will receive an IP address
and a port number for dispatching the actual HTTP request towards the service.
Optionally, DNS caching can be enabled so that the result of a lookup will be
cached by a requesting service for a set amount of seconds.
Consequently, the software developers or network administrators have to
create a configuration file for every service in the architecture to specify
their IP address and port number next to other information which are optional.
However, in reality the services will register themselves with their configuration
towards \textit{Consul} if correctly implemented;
\textit{Consul} offers endpoints to let services register themselves.
Moreover, health checks are a native and important feature of \textit{Consul}
as well. For each service a custom health check can be provided that
will be run in a set interval. A basic example would be that \textit{Consul}
sends a HTTP GET request to the service every ten seconds;
once a request is not successful, the health check fails and \textit{Consul}
automatically removes that service from the \textit{Service Discovery}
until the health checks are passing again. Thereby, the broken service will
not be hit by even more unsuccessful requests by other services which would just
prolong the downtime. Instead, the DNS lookup returns an error and the
requesting service can handle that case accordingly~\cite{consul}.\\
In summary, with a \textit{Service Discovery} each service just has to
make requests towards a never changing address which will be resolved by
\textit{Consul} automatically. Thus, the implementation inside of a service is
are a lot more concise since no knowledge about the structure of the network
is needed at no point during its runtime.

\subparagraph{API Gateway and Scooter Gateway}
The \textit{API Gateway} has two tasks in this architecture.
Firstly, it serves as a literal gateway to shield all the services from
the outside and especially from the HTTP requests of the users.
This is mainly a security measurement so the services do not have to
implement authorization each on their own. Instead, the \textit{API Gateway}
checks that every user is authenticated and authorized to access certain
API endpoints. Thus, the services just accept API calls coming directly from
the \textit{API Gateway} to ensure that they have been verified to be eligible
for further handling.\\
Secondly, it provides the API endpoints that are actually available from the
outside as each service has their own host address and port number.
Thus, without the \textit{API Gateway} these information would need to be
directly exposed towards the front-end applications. Moreover, with cloud-based
deployments they might change in the future which would make the information
outdated on the client side.

Similarly, the \textit{Scooter Gateway} fulfills the same task towards the
fleet of scooters. The other services can just pass the
information they want to dispatch to a certain scooter to the
\textit{Scooter Gateway} which then sends the actual request to the scooter.
In this way, the services do not need to hold information on the host addresses
and port numbers of each scooter which makes the handling of them through
a shared gateway tremendously easier. Otherwise, the information would need
to exist redundantly as well since currently two different services are
dispatching requests towards scooters.\\

In summary, figure \ref{fig:service-interaction} portrays an exemplary interaction of the
\textit{API Gateway}, the \textit{Service Discovery} and two services themselves.

\begin{figure}[htb]
\centering
\includegraphics[width=.55\textwidth]{service-interaction}
\caption{Exemplary functional interaction of API Gateway, Service Discovery, and two services}
\label{fig:service-interaction}
\end{figure}

\paragraph{API design} \label{construction-api_design}
There are many ways to model an API to make functionality of a service accessible
to other ones. The most basic one is indeed a normal HTTP API preferably with
JSON as the data exchange format; this is sometimes also called \textit{JSON over HTTP}.
\textit{JSON over HTTP} is both easy to setup as well as easy to use;
nevertheless, the handling is not very native to any programming language as one
has to dispatch a HTTP request for every function call towards another service.
A developer could hide that functionality behind an API wrapper so within the
source code the developers can access the other services API just like a normal
function call. However, this necessitates an API wrapper for every single service.
Moreover, the wrappers would have to be cloned onto every other service;
the same is true for updates to the wrappers.
As a consequence, this API design is just available as a fallback procedure
in case another component cannot use a more advanced technique e.g. a monolithic
legacy software solution.

The just mentioned more advanced technology is RPC which stands for
\textit{Remote Procedure Calls}.
It lets the developers use the API of another service just like a procedure within
its own but remotely. Yet, RPC always needs a codec which is essentially the
data exchange format of the calls. The preferred one of \textit{Go Micro}
is \textit{Protocol Buffers}, an open format developed and maintained by Google.
It it also especially well suited for this thesis at it not only offers a
data exchange format but also an \textit{Interface Description Language (IDL)}
to model the API of a service. Google additionally offers \textit{protoc},
a tool to automatically generate the API out of a \textit{Protocol Buffers}
definition~\cite{protobuf}. The tool works with many programming languages
which would make the integration into legacy software all the easier.

\paragraph{Runtime configuration}
As mentioned in the subsection on the \textit{\nameref{subsect:app-requirements}}
of the designed application, there are a couple of requirements that
mandate an application-wide mechanism to configure certain values e.g.
the amount of time a reservation of a scooter stays valid for.
For that purpose, a configuration server as a central source for all the
configuration data is contrived. That server will broadcast a configuration file
upon a request by a service. The services should implement that they request
the current configuration from said server upon their startup. Consequently,
a change to the configuration will not be adopted until the affected services
are getting restarted. Yet, this is not a big problem since changes to the proposed
application-wide configuration tantamount to a vast change in how the
short-term rental process works. Thus, a restart of each affected service is
necessary anyways to avert inconsistently configured services.

\paragraph{Concurrency}
The term \textit{Concurrency} describes the way of running multiple function
calls simultaneously opposed to a sequential approach that many programming languages
pursue; this concept is also called \textit{asynchronous execution}.
While the \textit{Go} programming language is also executed synchronously,
it offers built-in concurrency support as well~\cite{golang-concurrency}.
Each remote call from another service is handled as an asynchronous one already
by \textit{Go Micro} or even a normal HTTP server. On top of that, within one
of those remote calls multiple simultaneous functions can be invoked via
the built-in \textit{goroutines} which is the name of the concurrency feature
of the \textit{Go} programming language. However, this is optional and is only
utilized when applicable.

\paragraph{Assertions and \textit{Defensive Programming}}
Assertions are essentially built into the \textit{Go} programming language
since it is strictly typed. Effectively, every input parameter into any
function is getting type-checked. Thus, a function cannot run into an error
based on a erroneous input type as the whole program will not compile
if one is present. For instance, if a function expects a string as its input,
it cannot be called with a number.

Furthermore, a \textit{Circuit Breaker} should be used to prevent large-scale failures.
If one service is experiencing high load which leads to high latency on incoming
requests from other services or even a complete breakdown of said service,
the requesting services might themselves become sluggish owing to the fact
that they will wait for the responses of the broken service.
As a consequence, this can lead to a cascade of breakdowns of services that wait for
already broken ones to handle their requests. To mitigate that problem,
the aforementioned \textit{Circuit Breaker} pattern can be utilized.
Critical requests towards other services are getting invoked through
a \textit{Circuit Breaker} which will prevent subsequent requests to
a service that recently was not able to respond within a set timeout.
In that way, the broken service can recover from the high load
while the one where the \textit{Circuit Breaker} tripped will not
break down itself~\cite{fowler-circuit-breaker}.

\paragraph{Error handling}
\textit{Go Micro} offers built-in methods to retry calls towards
other services in case of a failure~\cite{go-micro}. If all of them still fail,
an error is logged to investigate the malfunction later on.



\chapter{Implementation} \label{chap:implementation}

\epigraphhead[55]{\epigraph{Almost all of contemporary software engineering
is focused on the singular goal of improving time-to-market.
Microservices are an evolution of the service-oriented architecture pattern
that elegantly eliminate organizational friction […]}
{\textit{Peter Bourgon}}}


\section{Project structure}

The source code of the project consist of four directories:
\begin{itemize}[noitemsep]
\item \code{api-gateway}
\item \code{common}
\item \code{services}
\item \code{vehicle-gateway}
\end{itemize}
The \code{common} directory contains singular functions that are general
enough to be shared by multiple services. Owning to the fact that directly in the
\code{services} directory each service has its own directory, this structure
to share source code is the most consistent one.

The name of the project is \textit{Toranos} which is the name of the
Celtic god of thunder and the wheel --- a fitting name for the purpose of the application.
The project is licensed as \textit{Open Source} software under the
\textit{Apache 2.0} license. The source code is freely available on GitHub at:\\
\code{\url{https://github.com/loehnertz/Toranos}}


\section{Runtime configuration}

The \code{config} module of \textit{Go Micro} offers an easy integration
of a configuration store into the service-oriented architecture.
A key-value store is used to persist the configuration.
This approach was chosen to allow for easy configuration by administrators
that does not mandate a recompilation of the source code.
The utilized \textit{Service Discovery} tool \textit{Consul} additionally offers
such a key-value store. Thus, the configuration is directly set within the
web interface of \textit{Consul}. Alternatively, the API of \textit{Consul}
can be used via HTTP or the command-line to do the same.
\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{consul-kv-store}
\caption{The web interface of the key-value store of \textit{Consul}}
\label{fig:consul-kv-store}
\end{figure}
Upon the startup of each service, \textit{Consul} is queried to retrieve the
configuration. Thus, a change within the configuration only takes effect
after the affected services are restarted. This should not be a problem however
since many changed settings alter processes so much that a restart might
have been necessary anyways to avoid data inconsistencies.
Some helper functions were created to use the configuration during runtime.

\begin{lstlisting}[title=services/billing/main.go]
var conf config.Config

// Initialize the configuration
conf = common.InitConfig()

// Retrieve a setting via its path
common.GetConfigStringByPath(conf, "service-names", "billing")
\end{lstlisting}

The \code{conf} object is initialized only once during the startup of each service
to avoid the aforementioned inconsistencies and to reduce the load
towards \textit{Consul}. The helper function \code{InitConfig()} retrieves
the whole configuration from \textit{Consul} in a format that allows for the
path-based querying seen in the last source code snippet.

\begin{lstlisting}[title=common/config.go]
func InitConfig() config.Config {
	// Initialize Consul as a source
	consulSource := consul.NewSource(consul.WithPrefix("/"))

	// Create new config
	conf := config.NewConfig()

	// Load source
	conf.Load(consulSource)

	return conf
}
\end{lstlisting}

However, the configuration is not restricted to \textit{Consul};
every other key-value store or even a plain JSON file could be used
for that purpose in the same manner due to the versatile nature of \textit{Go Micro}.


\section{Service structure}

The source code structure of each is service is very similar.
Inside of its directory there is always a \code{main.go} file and a \code{proto}
directory.

Inside of \code{main.go} at least a function with the identifier
\code{main()} has to exist. Additionally, the package of that file is always
\code{package main} as well. These naming conventions are required by the
\textit{Go} programming language. Only a function with that name inside of
a package named \code{main} can act as the starting point of any program.

\subsection{Service definition} \label{subsect:service-definition}
Furthermore, the \code{proto} directory comprises three files.
One contains the \textit{service definition} of that service and the other
two are \code{.go} files which are generated via the \textit{Protocol Buffers}
definition as stated in subsection \ref{construction-api_design}.

\begin{lstlisting}[title=services/fleet-monitor/proto/fleet-monitor.proto, language=protobuf3]
syntax = "proto3";

service FleetMonitor {
    rpc AvailableVehicles (Empty) returns (AvailableVehiclesResponse) {}
}

message Empty {}

message AvailableVehiclesResponse {
    message Vehicle {
        string VehicleId = 1;
        string Location = 2;
        uint32 ApproximateRadialRangeInMeters = 3;
    }
    repeated Vehicle vehicles = 1;
}
\end{lstlisting}

This is an example of a \textit{Protocol Buffers service definition}.
At the top, the version of \textit{Protocol Buffers} has to be set.
Secondly, the service is assigned a name and its RPC endpoints are described.
Within the first set of parentheses, the structure of the request has to be
defined. In this case, the request has no body so its \code{Message} is just an
empty one.

However, the response of this RPC comprises the currently available vehicles,
so the response just consists of a list of the \code{Message} of a \code{Vehicle}.
Using the directive \code{repeated}, an existing message is defined to be
handled as a list of indefinite length.
Finally, each \code{Message} consists of a number of fields. Those each have
a data type as well as an unique integer which is just used internally for
encoding purposes.

Using the command-line tool \textit{protoc}, readily available \code{.go} files
are generated which are filled with helper functions for creating
a server and a client for the service and each of its RPC endpoints.
For instance, the function \code{RegisterFleetMonitorHandler()} is generated
to register the newly defined service.
This is then done from within the \code{main()} function of each service.

\subsection{Service scaffolding}

\begin{lstlisting}[title=services/fleet-monitor/main.go]
var service micro.Service

func main() {
	// Create the service
	service = micro.NewService(
		micro.Name(config.FleetMonitorName),
		micro.RegisterTTL(time.Second*30),
		micro.RegisterInterval(time.Second*10),
	)
	service.Init()

    // Register the handler
	fleet_monitor.RegisterFleetMonitorHandler(
		service.Server(),
		new(FleetMonitor),
	)

    // Run the server
	if err := service.Run(); err != nil {
		panic(err)
	}
}
\end{lstlisting}

Every \code{main()} function inside of each service looks roughly similar.
Firstly, a new service is created via \textit{Go Micro};
\code{micro.NewService()} expects a name as the identifier.
This name is used to register the service toward the \textit{Service Discovery}.
Thus, the name is taken out of the configuration.
In essence, this approach was chosen so that other services can
faultlessly discover each created one by using the same name out of the configuration.

Secondly, an automatic health check is added as described in
\nameref{subparag:service-discovery}. These arguments register the service
at the \textit{Service Discovery} with the setting to check if the service is
still running every 30 seconds while re-registering it every ten seconds.
Utilizing this setting, the \textit{Service Discovery} and other
network components can react quickly to a sudden breakdown of a service e.g.
by starting a new instance of that service.

In the next step the handler which accepts incoming RPCs is initialized with the
aforementioned function at \code{fleet-monitor.micro.go}.
The argument \code{new(FleetMonitor)} is explained in subsection \ref{subsect:service-handlers}.

Finally, the service is started by running its server. Upon any errors regarding
the server component of the service that handles incoming requests,
the program will \code{panic} which means that it crashes properly.

Depending on the service, sometimes other components are initialized additionally.
A common example are connections towards a database or caching system.

\begin{lstlisting}[title=services/fleet-controller/main.go]
const DatabaseDriver = "postgres"
const DataSource = "user=jloehnertz dbname=toranos_fleet sslmode=disable"

var database *sql.DB

// Connect to the database
var databaseError error
database, databaseError = sql.Open(DatabaseDriver, DataSource)
if databaseError != nil {
	panic(databaseError)
}
\end{lstlisting}

\begin{lstlisting}[title=services/fleet-monitor/main.go]
var redisClient *redis.Client

// Initialize a Redis client for caching purposes
redisClient = commons.InitRedisClient(
                  config.RedisHostAddress,
                  "",
                  config.RedisDatabaseId,
              )
\end{lstlisting}

\subsection{Service handlers} \label{subsect:service-handlers}

As already depicted, each service leverages a helper function to initialize
its handler that processes each RPC. Effectively, the handler routes incoming
RPCs towards the corresponding functions that are assigned to handle it.
These functions oftentimes call another function that performs the actual work
which is located in another distinct file according to its purpose.

\begin{lstlisting}[title=services/fleet-monitor/main.go]
type FleetMonitor struct{}

func (fm *FleetMonitor) AvailableVehicles(
  ctx context.Context,
  req *fleet_monitor.Empty,
  res *fleet_monitor.AvailableVehiclesResponse
) error {
    // Call the designated function
	res.Vehicles = retrieveAvailableVehicles()

    // Return no error
	return nil
}
\end{lstlisting}

This example is taken out of the \code{fleet-monitor} service which has
exactly one RPC endpoint to retrieve all vehicles that are currently available
to rent. This is another point where the \textit{Protocol Buffers service definitions}
become useful. The request and response objects are input arguments to that
handler function. It gets called by the handler that \textit{Go Micro} established via\\
\code{RegisterFleetMonitorHandler()}.

The function \code{AvailableVehicles()} is a method of the empty\\
\code{FleetMonitor} \code{struct}. Methods in the \textit{Go} programming
language are written by prepending the name of the \code{struct} that can
call that method. The method can then be called with simple \textit{dot-notation}:\\
\code{FleetMonitor.AvailableVehicles(ctx, req, res)}.
Therefore, \\\code{RegisterFleetMonitorHandler()} receives a new instance of that\\
\code{FleetMonitor} \code{struct} with \code{new(FleetMonitor)} which enables
the handler to call each handler function upon an incoming RPC.
The \code{context} argument is used for additional meta-information about the RPC.

The most important aspect to take note of are the arguments of the handler function.
There types are taken out of the generated helper files based on the service definition.

\begin{lstlisting}[title=services/fleet-monitor/proto/fleet-monitor.pb.go]
type AvailableVehiclesResponse struct {
	Vehicles []*AvailableVehiclesResponse_Vehicle
}

type AvailableVehiclesResponse_Vehicle struct {
	VehicleId                      string
	Location                       string
	ApproximateRadialRangeInMeters uint32
}
\end{lstlisting}

In this exact case, the \code{repeated} directive exhibited in subsection
\ref{subsect:service-definition} can be seen in action;
\code{AvailableVehiclesResponse} just has a reference to a \code{slice} of
% The second code snippet actually has to be \code{AvailableVehiclesResponse_Vehicle}
\code{AvailableVehiclesResponse} which corresponds to a list in \textit{Go}.

Using these structs, the request as well as the response of each RPC
are strictly defined which makes incorrect usage by a developer nearly impossible
as the compiler of \textit{Go} would directly halt with an error during the
compilation process.
The important detail to note is that this would prevent the service from
even being deployed and executed on a server; the error is detected during
the development which reduces errors during runtime by a large margin.
Thus, the types of every field of the request and the response will be checked
during the compilation. Moreover, the names of the fields get restricted
to the defined ones.
However, at this point the exact implementation of the function
\code{retrieveAvailableVehicles()} is excluded for complexity reasons.
Nevertheless, a type check of the return value of that function which is then
passed into the response of the RPC via \code{res.Vehicles} is conducted as well.

\subsection{Service consumers}

Furthermore, the \code{main()} function of each service also sets up
the connection towards other services that need to be called by that service.
During the generation of the helper functions a function for that exact purpose
is created for each distinct service.
For example, the function \code{NewFleetMonitorService()} can be executed by
other services to connect to the \code{fleet-monitor} service. After that
connection is established, RPC endpoints can be called freely.

\begin{lstlisting}[title=services/statistics/main.go]
// The variables that will hold the references
var service micro.Service
var fleetControllerService fleet_monitor.FleetControllerService

// Create the service
service = micro.NewService(
  micro.Name(config.StatisticsName),
  micro.RegisterTTL(time.Second*30),
  micro.RegisterInterval(time.Second*10),
  micro.WrapClient(hystrix.NewClientWrapper()),
)
service.Init()

// Create the service's client
serviceClient := service.Client()
serviceClient.Init(client.Retries(3))

// Initialize the connection towards another service
fleetMonitorService = fleet_monitor.NewFleetControllerService(
                          config.FleetControllerName,
                          *serviceClient,
                      )
\end{lstlisting}

\textit{Go Micro} handles the first connection towards the
\textit{Service Discovery} that conducts a DNS lookup which is mandatory
for performing the actual RPC towards the obtained IP address and port number
of the desired service.
This is achieved by passing the name of the service into the helper function
as well as the client component of the service that performs the calls.

Moreover, during the setup of each service the client can be optionally equipped
with certain settings. As discussed in section \ref{subsect:analysis-construction},
a service will utilize a circuit breaker together with a retry mechanism to
avoid large-scale failures; this is accomplished with
the lines \textit{9} and \textit{15}.

The actual RPC is performed with a helper function again.

\begin{lstlisting}[title=services/statistics/bookings.go]
func retrieveAllBookingsOfCustomer(customer string) (
    bookings []*statistics.RetrieveBookingsResponse_Booking,
    err error
  ) {
	   resBookings, errBookings :=
        fleetControllerService.RetrieveBilledBookingsOfCustomer(
            context.TODO(),
            &fleet_controller.RetrieveBilledBookingsOfCustomerRequest{
    		        UserId: customer,
    	    },
        )

    	if errBookings != nil {
    		log.Log(errBookings)
    		return
    	}

      // Function continues with returning the bookings
}
\end{lstlisting}

Once again, the \textit{Protocol Buffers} definition is leveraged to prepare
a request to perform the RPC towards the \code{fleet-controller} service.
Of course, the variable \code{resBookings}, which contains the response of the RPC,
is constructed according to the service definition.

Finally, within this last source code snippet the remote procedure calls (RPC)
can be observed. Compared to a traditional API call, no HTTP request has to be
performed. Instead, the function \code{RetrieveBilledBookingsOfCustomer()}
can be called like a normal function native to the package.
Only in the background the RPC is executed as described
in section \ref{subsubsect:architectural-styles}.
Additionally, compared to an API call via HTTP, it is ensured that the content
of the request is correct considering the keys and value types.

\subsection{Scheduled tasks}

Apart from RPCs that are indirectly initiated by users of the application,
some tasks are executed automatically in an asynchronous and scheduled manner.
Consequently, a task runner is necessary to do so.
For the application, normal \textit{cron jobs} are utilized.
The software utility \textit{cron} is originally derived from UNIX and can be
used to schedule and run recurring tasks~\cite{crontab}.
An implementation written in \textit{Go} is used within the application.

\begin{lstlisting}[title=services/fleet-monitor/bookings.go]
// Initialize all the tasks
scheduler := cron.New()

scheduler.AddFunc(
    config.CheckForExpiredReservationsInterval,
    checkForExpiredReservations,
)

// Start all the tasks
scheduler.Start()
\end{lstlisting}

The interval for this exemplary task is retrieved from the configuration.
During the runtime of the service, the tasks will automatically executed according
to the configured interval.
For instance, as seen in the source code snippet, this is used by the
\code{fleet-monitor} service to cancel active bookings which are expired.

\subsection{Databases} \label{subsect:databases}

In order to achieve a minimum of \textit{Coupling} between the services,
every service has its own database.
Consequently, if one service needs any information that another service's
database persists, there is no other option but an RPC.
This pattern is strictly followed within the application to thwart any
dependencies between services. For example, if the database of one service gets
corrupted others might still be able to continue their work.
Similarly, every cache is separated from others.

Although it might seem counterintuitive to let a service execute RPCs towards
other services for every piece of data that is not existent in its own database,
this approach actively prevents conditions where two services mutate the same
data entity and thus create a state of inconsistency.
Instead, only legitimate data ever hits a service by leveraging the service definitions.

Furthermore, database queries are decoupled from other simultaneously running tasks;
owing to the fact, that the database management system (DBMS) prevents
simultaneous mutations of the same data entity, it would be feasible
to run multiple instances of a service that sees many RPCs that just
retrieve or persist data from or to its connected database.
Thus, the load of a large amount of RPCs towards such a service could be balanced.

Finally, the use case of the application is not dependent on much
shared data which makes such an approach all the more reasonable.
In other scenarios, a synchronization of data between databases might be useful
to minimize the amount of RPC overhead.


\section{API gateway} \label{sect:api-gateway}

The \textit{API gateway} accomplishes two aims:
It shields the services from the outside so that no client on the world wide web
can directly access any of the services; this mechanism is in place mainly for
security reasons.
Thus, a firewall blocks every request towards all the services and just allows
the \textit{API gateway} to do so.

Similarly, the second aim is to provide user authentication
at a central point without the need for every service to implement it as well.
As a consequence, the services can be sure that no request towards them is unauthorized.
To achieve this, the \textit{API gateway} implements endpoints that are either
accessible by the public or only by registered and authorized users.

\paragraph{Scaffolding}
Generally, the \textit{API gateway} just offers normal HTTP endpoints that accept
JSON. Upon starting it, the endpoints are initialized.

\begin{lstlisting}[title=api-gateway/main.go]
func initRoutes(router *mux.Router) {
	// Public route
	router.Handle("/login", http.HandlerFunc(getAuthToken)).Methods("POST")

	// Authenticated route
	router.Handle(
      "/available-vehicles",
      authenticationMiddleware(http.HandlerFunc(availableVehicles)),
    ).Methods("GET")
}
\end{lstlisting}

For example, the endpoint at \code{/available-vehicles} calls the function
\code{availableVehicles()} upon an incoming \code{GET-request}.
Those handler functions perform regular RPCs towards the desired services just
as any other service of the application. In that sense, the \textit{API gateway}
is a service itself in a way.
The allowed HTTP methods are set by arguments of the \code{Methods()} method.
To control which endpoints are public and which are not, a middleware, as described
in subsection \ref{subsect:construction}, is utilized.

\paragraph{JSON Web Tokens}
The authentication is conducted via \textit{JSON Web Tokens} (JWT) which is
an RFC-standardized token format for enhanced authentication~\cite{jwt-rfc}.
Thus, a JWT has a similar use case as a \textit{session ID}.
However, the standard comprises of many more features.
Upon issuing a new JWT, a JSON object is created that includes information
like the username of the user, its role (e.g. customer, administrator), an
expiration date and even completely arbitrary data.
The resulting JSON object gets serialized and signed with an asymmetric
private key. Consequently, the client can verify the authenticity of the token
while any tampering of the token can be directly revealed since the signature becomes
invalid. Furthermore, the expiration date of the token is automatically checked
when checking its validity. Thus, no expiration dates have to be saved to
a database anymore as it is usual with \textit{session IDs}.

\paragraph{Authentication middleware}
The \code{authenticationMiddleware()} is just another function that is executed
before the function that implements the business logic.
It can execute any code and either pass the request to the function that should
handle the request or directly return a response.

\begin{lstlisting}[title=api-gateway/middlewares.go]
func authenticationMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		token := strings.Split(r.Header.Get("Authorization"), " ")

		// Check if the user is authenticated
		resAuthenticateUser, errAuthenticateUser :=
      userManagementService.AuthenticateUser(
          context.TODO(),
          &user_management.AuthenticateUserRequest{Token: token[1]},
      )

		if errAuthenticateUser != nil {
			log.Log(errAuthenticateUser)
			w.Write([]byte(commons.UnknownError.Error()))
		} else if resAuthenticateUser.Authenticated {
			// Call the next handler
			gorillacontext.Set(r, "user", resAuthenticateUser)
			next.ServeHTTP(w, r)
		} else {
			w.Write([]byte(commons.NotAuthorizedError.Error()))
		}
	})
}
\end{lstlisting}

Any user can login with their credentials to receive a new, valid JWT.
With every subsequent request towards the public API, the JWT is part of the
HTTP request as the \code{Authorization} header.
In the case of the \code{authenticationMiddleware()}, the middleware performs
a regular RPC towards the \code{user-management} service to check if the
token of the requesting user is valid. The \code{user-management} service also
implements an endpoint to issue new tokens among others which is used for logging
a user in.
If the service confirms the validity of the token the current request is passed
to the handler function. If the token is invalid, the middleware directly responds
with an error.
The handler functions then call other RPC endpoints according to their functionality
and return adjusted responses to the requesting users.

Due to the fact, that the JWT includes the username and even the role of the
requesting user, it is not even mandatory anymore to include the username with
each request as all this information is an integral part of the JWT.
Moreover, to check if a user is authorized to request certain endpoints no
database has to be queried; the role of the user is encoded in the JWT and
by signing the token all its information are tamper-proof.

\paragraph{Resilience measures}
Finally, the \textit{API gateway} implements certain measures to enhance
the resilience of the whole application.
According to the paradigm of \textit{Defensive Programming} which was introduced
in subsection \ref{subsect:construction}, the gateway leverages a library
developed by \textit{Netflix Inc.} called \textit{Hystrix}~\cite{hystrix}.
It acts similar to a middleware by wrapping every RPC of the
\textit{API gateway} towards another service.
In fact, \textit{Go Micro} even offers a method to directly do so with
supported wrappers by calling \code{micro.WrapClient()} during the startup
of the \textit{API gateway}.

\begin{lstlisting}[title=api-gateway/main.go]
// Create the service
service = micro.NewService(
	micro.Name(config.ApiGatewayName),
	micro.RegisterTTL(time.Second*30),
	micro.RegisterInterval(time.Second*10),
	micro.WrapClient(hystrix.NewClientWrapper()), // Initialize Hystrix
)
\end{lstlisting}

\textit{Hystrix} offers three distinct features:
a custom timeout for every RPC, a capped pool of concurrent connections towards
a service as well as a \textit{circuit breaker}.
All of these measures improve the overall resilience of the application
as well as its average response times by not overloading any service with incoming
requests.


\section{Vehicle gateway}

The \textit{Vehicle gateway} fulfills a similar task as the \textit{API gateway}.
Whenever a service needs to communicate with a vehicle of the fleet, a RPC
is conducted towards the \textit{Vehicle gateway}. The gateway then sends a request
to the selected vehicle by using its host address that has been saved to the
database of the gateway beforehand.
However, this component of the application is only implemented as a stub
as the technical specifications of an actual fleet of vehicles is not part
of this thesis which would make a proper implementation not feasible.



\chapter{Evaluation} \label{chap:evaluation}

\section{Scalability of the application}




\section{Extensibility of the application}

Due to the architecture of the application, extending the existing solution
with new features is simple.
If the desired functionality is large enough a completely new service might
be suitable if the existing landscape facilitates its addition.
In that case, the service first needs to be designed just as any other as well.
In the second step, a \textit{Protocol Buffers} service definition needs to be
created according to the resulted design.
Afterwards, the construction of the actual business logic of the service is
conducted just as it would be with any other software architecture.
The main difference is that each RPC endpoint needs to be created and connected
to the business logic.

Furthermore, owing to the fact that \textit{Protocol Buffers} support many
programming languages it would even be feasible to construct a service in a
completely different language than the existing ones.
Using \textit{protoc} helper functions can be created not only for the
\textit{Go} programming language but many others like \textit{C++}, \textit{C\#},
\textit{Java}, or \textit{Python}~\cite{protobuf}.
Similarly, the database does not have to be SQL-based for every service;
if a NoSQL database, a key–value store or even a completely different
database design are more suitable for the new service, they can be easily
used as described in subsection \ref{subsect:databases}.

Finally, the service needs to be added to the configuration and its RPC endpoints
to other services that should consume the new one.
Alternatively, if RPC endpoints of the service should be directly usable
by the users of the application, they have to be connected to the
\textit{API gateway} which is exhibited in great detail in section \ref{sect:api-gateway}.

On the other hand, any service can be extended of course.
Interestingly, the procedure would be very similar to adding a completely new service.
Presumably, a new RPC needs to be defined that can be integrated into the service
as just described.


\section{Extension of existing legacy software}

\paragraph{Foundation}
Generally speaking, for the integration of a service-oriented architecture with
an existing legacy software solution, two facets have to be discerned:
letting the legacy software solution use functionalities of the services and
letting the services use functionalities of the legacy software solution.
The latter one is significantly more difficult since many legacy software solutions
completely lack an externally accessible API.

Nevertheless, this fact is not a problem in itself.
As exhibited in subsection \ref{issues-legacy-software}, legacy software generally
engenders many problems while the main reasons to continue using it are of
economic nature as depicted in subsection \ref{sub-sect:continue-legacy}.

The hypothetical integration in this section focuses on an incremental transition
from the legacy solution to a service-oriented architecture.
Although, the application developed during the course of this thesis is nearly
fully-featured, oftentimes an incremental integration is more feasible due to
the just mentioned economic difficulties of subsection \ref{sub-sect:continue-legacy}.

Thus, the focus should be on implementing an increasing amount of functionalities
within the service-oriented architecture. Consequently, only the first of the
aforementioned facets of an integration is discussed in this section.
It would be counterproductive to let the services use any functionality of the
legacy software solution since this creates a dependency.
Instead, the legacy software solution should substitute an increasing amount
of its own implementation with newly implemented services.

Thus, one of the exhibited solutions to legacy software issues from subsection
\ref{solutions-issues-legacy-software} can be applied.
To summarize, the discussed \textit{Migration} or to be more exact,
a \textit{Reengineering} approach is congruent with the described procedure.

\paragraph{Implementation}
Two options exist to actually integrate the service-oriented architecture
with an existing legacy software solution.
Based on the \textit{Protocol Buffers} service definition, RPC connectors can
be generated for many programming languages. Those can be integrated into any
software solution, they are not limited to a service-oriented architecture.
The legacy software solution could then utilize the RPCs just as the other
services do as well. Although this approach is the more cleaner one of the two,
since it will be guaranteed that the RPCs are well-formed, it can either be
that no RPC connectors can be generated for the programming language the
legacy solution uses or that an integration is too difficult due to its
architecture. For example, if the \textit{Protocol Buffers} service definitions
dictate a certain data format that is too different from the one currently
existing within the application, an adjustment would involve too much work.
This is especially unpreferable since the available developers should use their
time on modernizing the architecture as exhibited in the last paragraph and
not on adapting the legacy software solution which should be discontinued anyways.

Nonetheless, a second approach exists that is suitable for nearly every
web-based application --- ordinary HTTP requests.
\textit{Go Micro} offers an \code{api} module to do just that without much
setup. The \code{api} module is started via a command-line with
\code{micro api --address localhost:8123} which binds it to a certain port number.
Afterwards, any client with access to that port can invoke fake RPC via a regular
HTTP request towards the \textit{Go Micro} API via the endpoint \code{/rpc}.
Consequently, a firewall is mandatory to shield off unwanted access which
would completely remove the protection the \textit{API gateway} introduced.
Internally, the \textit{Go Micro} API dispatches a real RPC and returns the
result to the requesting client. The format of such a request is JSON with
a certain format.

\begin{lstlisting}[title=\textit{Go Micro} API request]
{
	"service": "fleet-controller",
	"method": "FleetController.Book",
	"request": {
		"vehicleId": "2f4d0a",
		"customerId": "458921"
	}
}
\end{lstlisting}



\chapter{Future Work} \label{chap:future-work}

\section{Deployment}

The aspect of deploying the developed application to a remote server is
not part of this thesis due to the complexity of the subject.
Nevertheless, a cloud-based, containerized solution is preferred since
each service runs completely independently. The aspect of scaling the application
to meet the existing load is more suitable when new instances of a service
can be created without any hassle. Furthermore, dynamic scaling of the amount
of instances based on current load levels is a field of interest for a
service-oriented application as well.


\section{Connecting a front-end}

The application as a whole is not really usable without a connected front-end.
Owing to the fact, that the solution is geared towards regular customers
and no enterprise personnel, the API by itself will not be used by any customer.
Since the API works with JSON, a mobile application as well as a web-based one
are both feasible.


\section{Extending actual legacy software}

As discussed earlier, the corporate partner of this thesis acquired a
software solution for the purpose of the short-term rental of electric scooters.
Consequently, the next step would be to evaluate which components of that
legacy software solution are possibly outdated and would therefore benefit
from an incremental modernization via a service-oriented architecture.
Moreover, some functionalities might be missing entirely which could be added
to the system by the means of services.



\chapter{Conclusion} \label{chap:conclusion}





\newpage

% Source lists
\listoffigures
% \listoftables
\newpage

% Separate the sources with 'bibtopic'
\bibliographystyle{plain}
\begin{btSect}{references}
\section*{References}
\btPrintCited
\end{btSect}
\begin{btSect}{online}
\section*{Online Sources}
\btPrintCited
\end{btSect}

\end{document}
